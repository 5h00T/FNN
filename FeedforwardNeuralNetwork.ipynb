{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN():\n",
    "    \"\"\"\n",
    "    入力、隠れ、出力層の順伝播型ニューラルネットワーク\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate, batch_size):\n",
    "        # 入力層、隠れ層、出力層のノード数を設定\n",
    "        self.inodes = inputnodes\n",
    "        self.hnodes = hiddennodes\n",
    "        self.onodes = outputnodes\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # 平均0、標準偏差1の正規分布に従う\n",
    "        # self.w2 = np.random.randn(self.hnodes, self.inodes)\n",
    "        # self.b2 = np.zeros((self.hnodes, 1))\n",
    "        # self.w3 = np.random.randn(self.onodes, self.hnodes)\n",
    "        # self.b3 = np.zeros((self.onodes, 1))\n",
    "\n",
    "        # Xavierの初期値\n",
    "        self.w2 = np.random.normal(0.0, pow(1 / self.inodes, 0.5), (self.hnodes, self.inodes))\n",
    "        self.b2 = np.zeros((self.hnodes,1))\n",
    "        self.w3 = np.random.normal(0.0, pow(1 / self.hnodes, 0.5), (self.onodes, self.hnodes))\n",
    "        self.b3 = np.zeros((self.onodes,1))\n",
    "\n",
    "        # 学習率の設定\n",
    "        self.lr = learningrate\n",
    "\n",
    "    def train(self, inputs_list, target_list):        \n",
    "        inputs = np.array(inputs_list).reshape( self.inodes, -1)\n",
    "        targets = np.array(target_list).reshape(-1,self.onodes)\n",
    "\n",
    "        z2, u2, z3, loss = self.forward(inputs_list, target_list, train=True)\n",
    "        print(\"loss\", loss)\n",
    "\n",
    "        o = z3\n",
    "\n",
    "        # deltaの計算\n",
    "        # 出力層の誤差 = (最終出力 - 目標出力)\n",
    "        delta_3 = o - targets.T\n",
    "        delta_2 = self.d_sigmoid(u2) * (np.dot(self.w3.T, delta_3))\n",
    "\n",
    "        #　重みを更新\n",
    "        self.b3 += self.lr * (-np.dot(delta_3, np.ones((self.batch_size,1))))\n",
    "        self.w3 += self.lr * (-np.dot(delta_3, z2.T))\n",
    "        self.b2 += self.lr * (-np.dot(delta_2, np.ones((self.batch_size,1))))\n",
    "        z1 = inputs\n",
    "        self.w2 += self.lr * (-np.dot(delta_2, z1.T))\n",
    "\n",
    "        return z3, loss\n",
    "\n",
    "\n",
    "    def forward(self, inputs_list, target_list=None, train=False):\n",
    "        \"\"\"\n",
    "        順方向計算\n",
    "        \"\"\"\n",
    "        # 入力リストを行列に変換\n",
    "        inputs = np.array(inputs_list).reshape( self.inodes, -1)\n",
    "\n",
    "        # 隠れ層\n",
    "        u2 = np.dot(self.w2, inputs) + np.dot(self.b2, np.ones((inputs.shape[1],1)).T)\n",
    "\n",
    "        # 隠れ層で結合された信号を活性化関数により出力\n",
    "        z2 = self.sigmoid(u2)\n",
    "\n",
    "        # 出力層\n",
    "        u3 = np.dot(self.w3, z2) + np.dot(self.b3, np.ones((inputs.shape[1],1)).T)\n",
    "\n",
    "        # 出力層で結合された信号を活性化関数により出力\n",
    "        z3 = self.softmax(u3)\n",
    "\n",
    "        if target_list is not None:\n",
    "            targets = np.array(target_list).reshape(-1,self.onodes)\n",
    "            loss = self.cross_entropy_error(z3,targets)\n",
    "            if train:\n",
    "                return z2, u2, z3, loss \n",
    "            return z3, loss\n",
    "\n",
    "        return z3\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def d_sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        シグモイド関数の導関数\n",
    "        \"\"\"\n",
    "        return self.sigmoid(x)*(1 - self.sigmoid(x))\n",
    "\n",
    "    def softmax(self, x):\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y\n",
    "\n",
    "\n",
    "    def cross_entropy_error(self, y, t):\n",
    "        \"\"\"\n",
    "        交差エントロピー誤差を計算する\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : numpy.ndarray\n",
    "            ニューラルネットの出力\n",
    "\n",
    "        t : numpy.ndarray\n",
    "            教師データ(one-hot表現)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        error : numpy.ndarray\n",
    "            計算した誤差\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = y.shape[1]\n",
    "\n",
    "        # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\n",
    "        if t.size == y.size:\n",
    "            t = t.argmax(axis=1)\n",
    "\n",
    "        delta = 1e-7\n",
    "        y = y.T\n",
    "        error = -np.sum(np.log(y[np.arange(batch_size), t] + delta)) / batch_size\n",
    "\n",
    "        return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_nodes = 784  #  mnistの場合28*28=794\n",
    "hidden_nodes = 100  # 隠れ層のサイズ\n",
    "output_nodes = 10  # mnistの場合0 ~ 9\n",
    "\n",
    "learning_rate = 0.01  # 学習率\n",
    "\n",
    "batch_size = 100\n",
    "epoch = 10  # 学習回数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = FNN(input_nodes, hidden_nodes, output_nodes, learning_rate, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mnistデータセットのダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://pjreddie.com/media/files/mnist_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://pjreddie.com/media/files/mnist_train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mnist_train.csv\", 'r') as f:\n",
    "    training_data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mnist_test.csv\", 'r') as f:\n",
    "    test_data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(training_data))  # データ数の確認‘\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = np.array(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loss = []  # 学習誤差\n",
    "test_loss = []  # テスト誤差\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "for i in range(epoch):\n",
    "    loss = 0\n",
    "    score = 0\n",
    "    idx = np.random.permutation(len(training_data))\n",
    "    print(\"epoch:\", i)\n",
    "    for j in range(0,60000, batch_size):  # ランダムに並べ直したすべてのデータで学習する\n",
    "        data = training_data[idx[j:j+batch_size]]\n",
    "        x_batch = []\n",
    "        t = []\n",
    "        for k in data:\n",
    "            x_batch.append(list(map(lambda x: int(x) , k.split(\",\")[1:])))\n",
    "            t.append(int(k.split(\",\")[0]))\n",
    "        x_batch = np.array(x_batch).T / 255.0\n",
    "        y_batch = [ int(k[0]) for k in data]\n",
    "        \n",
    "        target = np.zeros(( batch_size, output_nodes))\n",
    "        \n",
    "        # one-hot表現に変更\n",
    "        for k in range(batch_size):\n",
    "            target[k, y_batch[k]] = 1\n",
    "            \n",
    "        # 学習\n",
    "        output, loss_ = network.train(x_batch, target)\n",
    "        # 訓練誤差と正解率を格納\n",
    "        loss += loss_\n",
    "        labels = np.argmax(output.T, axis=1)\n",
    "        score += np.sum(labels == t)\n",
    "        \n",
    "    train_loss.append(loss/(60000/batch_size))\n",
    "    train_accuracy.append(score / 60000)\n",
    "    \n",
    "    # テスト誤差を求める\n",
    "    loss = 0\n",
    "    score = 0\n",
    "    test_size = 10000\n",
    "    for j in range(0,test_size,batch_size):\n",
    "        data = test_data[j:j+batch_size]\n",
    "        \n",
    "        x_batch = []\n",
    "        t = []\n",
    "        for k in data:\n",
    "            x_batch.append(list(map(lambda x: int(x) , k.split(\",\")[1:])))\n",
    "            t.append(int(k.split(\",\")[0]))\n",
    "        x_batch = np.array(x_batch).T / 255.0\n",
    "        y_batch = [ int(k[0]) for k in data]\n",
    "        \n",
    "        target = np.zeros(( batch_size, output_nodes))\n",
    "        \n",
    "        # one-hot表現に変更\n",
    "        for k in range(batch_size):\n",
    "            target[k, y_batch[k]] = 1\n",
    "        output, loss_ = network.forward(x_batch, target)\n",
    "        loss += loss_\n",
    "        labels = np.argmax(output.T, axis=1)\n",
    "        score += np.sum(labels == t)\n",
    "        \n",
    "    test_loss.append(loss/(test_size/batch_size))\n",
    "    test_accuracy.append(score / test_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss[:], label=\"train_loss\")\n",
    "plt.plot(test_loss[:], label=\"test_loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_accuracy, label=\"train_accuracy\")\n",
    "plt.plot(test_accuracy, label=\"test_accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mnist_test.csv\", 'r') as f:\n",
    "    test_data = f.readlines()\n",
    "\n",
    "score = 0\n",
    "test_size = 10000\n",
    "for i in range(0,test_size,batch_size):\n",
    "    data = test_data[i:i+batch_size]\n",
    "    \n",
    "    x_batch = []\n",
    "    t = []\n",
    "    for j in data:\n",
    "        x_batch.append(list(map(lambda x: int(x) , j.split(\",\")[1:])))\n",
    "        t.append(int(j.split(\",\")[0]))\n",
    "    x_batch = np.array(x_batch).T / 255.0\n",
    "    \n",
    "    output = network.forward(x_batch)\n",
    "    labels = np.argmax(output.T, axis=1)\n",
    "    score += np.sum(labels == t)\n",
    "\n",
    "\n",
    "print(\"正解率 =\", score / test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mnist_train.csv\", 'r') as f:\n",
    "    training_data = f.readlines()\n",
    "    \n",
    "score = 0\n",
    "test_size = 60000\n",
    "for i in range(0,test_size,batch_size):\n",
    "    data = training_data[i:i+batch_size]\n",
    "    \n",
    "    x_batch = []\n",
    "    t = []\n",
    "    for j in data:\n",
    "        x_batch.append(list(map(lambda x: int(x) , j.split(\",\")[1:])))\n",
    "        t.append(int(j.split(\",\")[0]))\n",
    "    x_batch = np.array(x_batch).T / 255.0\n",
    "    \n",
    "    output = network.forward(x_batch)\n",
    "    labels = np.argmax(output.T, axis=1)\n",
    "    score += np.sum(labels == t)\n",
    "\n",
    "print(\"正解率 =\", score / test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### irisデータセット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = iris[\"target\"]\n",
    "data = iris[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:,0] /= np.max(data[:,0])\n",
    "data[:,1] /= np.max(data[:,1])\n",
    "data[:,2] /= np.max(data[:,2])\n",
    "data[:,3] /= np.max(data[:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.permutation(len(data))\n",
    "print(idx)\n",
    "train_x, train_y, test_x, test_y = data[idx[:100]], target[idx[:100]], data[idx[100:]], target[idx[100:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_nodes = 4  #  irisの場合4\n",
    "hidden_nodes = 4  # 隠れ層のサイズ\n",
    "output_nodes = 3  # irisdatasetの場合0 ~ 2\n",
    "\n",
    "learning_rate = 0.01  # 学習率\n",
    "\n",
    "batch_size = 10\n",
    "epoch = 500 # 学習回数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = FNN(input_nodes, hidden_nodes, output_nodes, learning_rate, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(x,y):\n",
    "    x_y = list(zip(x,y))\n",
    "    np.random.shuffle(x_y)\n",
    "    result_x, result_y = zip(*x_y)\n",
    "    return np.asarray(result_x), np.asarray(result_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "test_loss = []\n",
    "train_accuracy = []\n",
    "test_accuracy =[]\n",
    "\n",
    "for i in range(epoch):\n",
    "    loss = 0\n",
    "    score = 0\n",
    "    print(\"epoch:\", i)\n",
    "    train_x, train_y = shuffle(train_x, train_y)\n",
    "    for j in range(0,100, batch_size):  # 並び変えたすべてのデータで学習する\n",
    "        x_batch = train_x[j:j+batch_size]\n",
    "        y_batch = np.zeros(( batch_size, output_nodes))\n",
    "        t = train_y[j:j+batch_size]\n",
    "        \n",
    "        # one-hot表現に変更\n",
    "        for k in range(batch_size):\n",
    "            y_batch[k, train_y[j+k]] = 1\n",
    "        \n",
    "        # 学習\n",
    "        output, loss_ = network.train(x_batch.T, y_batch)\n",
    "        # 訓練誤差と正解率を格納\n",
    "        loss += loss_\n",
    "        labels = np.argmax(output.T, axis=1)\n",
    "        score += np.sum(labels == t)\n",
    "        \n",
    "    train_loss.append(loss/(100/batch_size))\n",
    "    train_accuracy.append(score / 100)\n",
    "    \n",
    "    # テスト誤差を求める\n",
    "    loss = 0\n",
    "    score = 0\n",
    "    test_size = 50\n",
    "    for j in range(0,test_size,batch_size):\n",
    "        x_batch = test_x[j:j+batch_size]\n",
    "        y_batch = np.zeros(( batch_size, output_nodes))\n",
    "        \n",
    "        # one-hot表現に変更\n",
    "        for k in range(batch_size):\n",
    "            y_batch[k, test_y[j+k]] = 1\n",
    "         \n",
    "        t = test_y[j:j+batch_size]\n",
    "        output, loss_ = network.forward(x_batch.T, y_batch)\n",
    "        loss += loss_\n",
    "        labels = np.argmax(output.T, axis=1)\n",
    "        score += np.sum(labels == t)\n",
    "    \n",
    "    test_loss.append(loss/(test_size/batch_size))\n",
    "    test_accuracy.append(score/test_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_accuracy, label=\"train_accuracy\")\n",
    "plt.plot(test_accuracy, label=\"test_accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss[:], label=\"train_loss\")\n",
    "plt.plot(test_loss[:], label=\"test_loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = 0\n",
    "test_size = 100\n",
    "for i in range(0,test_size,batch_size):\n",
    "    x_batch = train_x[i:i+batch_size]\n",
    "    t = train_y[i:i+batch_size]\n",
    "    output = network.forward(x_batch.T)\n",
    "    labels = np.argmax(output.T, axis=1)\n",
    "    score += np.sum(labels == t)\n",
    "\n",
    "print(\"正解率 =\", score / test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "score = 0\n",
    "test_size = 50\n",
    "for i in range(0,test_size,batch_size):\n",
    "    x_batch = test_x[i:i+batch_size]\n",
    "    t = test_y[i:i+batch_size]\n",
    "    output = network.forward(x_batch.T)\n",
    "    labels = np.argmax(output.T, axis=1)\n",
    "    score += np.sum(labels == t)\n",
    "\n",
    "print(\"正解率 =\", score / test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
